# -*- coding: utf-8 -*-
"""RAHF_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8Bug-l4fVCAXA7kLDJmhbKkN52jIsXm
"""
import json
import logging
import os

os.environ['NCCL_P2P_LEVEL'] = 'NVL'
import sys
import time

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import wandb
import yaml
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
from configs import get_args, VALID_SCORES
import random

sys.path.append('../')
from metrics import metric_cal_online
from criterions import MaskLoss, ScoreLoss
from dataset import RAHFDatasetCustom
from model.model_final import build_preprocessing_and_model
from utils import print_log_iter, save_in_training, final_save, print_trainable_params
from tqdm import tqdm
from accelerate import Accelerator
from model.ema import ExponentialMovingAverage

logger = logging.getLogger()


def set_params_trainable(model, args, stage='train'):
    if stage == 'warmup':
        if args.model == 'altclip':
            model.image_encoder.freeze()
            model.text_encoder.freeze()
        elif args.model == 'florence2':
            for param in model.florence2.parameters():
                param.requires_grad = False
    elif stage == 'train':
        if args.model == 'altclip':
            model.image_encoder.unfreeze()
            model.text_encoder.unfreeze()
        elif args.model == 'florence2':
            for name, param in model.named_parameters():
                if 'lora' in name:  # or 'vision_tower' in name:
                    param.requires_grad = True
        for name in args.freeze_params:
            for param in model.named_parameters():
                if name in param[0] or name == 'all':
                    param[1].requires_grad = False
        for name in args.unfreeze_params:
            for param in model.named_parameters():
                if name in param[0] or name == 'all':
                    param[1].requires_grad = True


@torch.no_grad()
def evaluate(model, dataloader, device, criterion, iter_counter, prefix='val', log_flag=True,
             threshold_list=None, accelerator=None):
    if threshold_list is None:
        threshold_list = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 85, 90, 95, 100, 105, 110]
    model.eval()
    loss_heatmap_im, loss_score_im, loss_heatmap_mis, loss_score_mis = 0, 0, 0, 0
    preds = {}
    gts = {}
    count = 0
    date_num = 0
    results_list = []
    with torch.no_grad():
        for inputs, targets in tqdm(dataloader, total=len(dataloader), desc=f'{prefix} evaluation'):
            count += 1
            inputs = inputs.to(device)
            pixel_values = inputs['pixel_values'].squeeze(1)
            if accelerator.state.mixed_precision == 'bf16':
                pixel_values = pixel_values.bfloat16()
            elif accelerator.state.mixed_precision == 'fp16':
                pixel_values = pixel_values.half()
            outputs_im = model(pixel_values, inputs['input_ids'][:, 0, :])  # implausibility
            output_heatmap, target_heatmap = outputs_im[0].to(device), targets['artifact_map'].to(device)
            output_score, target_score = outputs_im[1].to(device), targets['artifact_score'].to(device)
            if accelerator.state.mixed_precision == 'bf16':
                target_heatmap = target_heatmap.bfloat16()
                target_score = target_score.bfloat16()
            elif accelerator.state.mixed_precision == 'fp16':
                target_heatmap = target_heatmap.half()
                target_score = target_score.half()

            loss_heatmap_im += torch.nn.MSELoss()(output_heatmap, target_heatmap).item()
            loss_score_im += torch.nn.MSELoss()(output_score, target_score).item()

            img_names = targets['img_name']

            for idx, img_name_item in enumerate(img_names):
                format_list = ['.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.JPEG', '.bmp', '.BMP']
                for format in format_list:
                    if img_name_item.endswith(format):
                        img_name_item = img_name_item.split(format)[0]
                        break
                preds[img_name_item] = {
                    'pred_area': np.squeeze(np.uint8(output_heatmap[idx].float().cpu().numpy() * 255)),
                    'score': output_score[idx].item(),
                }
                gts[img_name_item] = {
                    'pred_area': np.squeeze(np.uint8(target_heatmap[idx].float().cpu().numpy() * 255)),
                    'score': target_score[idx].item(),
                }
        scale_factor = (255 ** 2, 4)
        # scale_factor = (255 ** 2, 1.)

    metrics_all = {}

    final_scores = {}
    for threshold in threshold_list:
        metrics = metric_cal_online(preds, gts, pred_mask_threshold=threshold)
        for key, value in metrics.items():
            if key == 'final_score':
                final_scores[threshold] = value
            if 'plcc' not in key.lower() and 'srcc' not in key.lower():
                key = f'{key}_{threshold}'
            key = f'{prefix}_{key}'
            metrics_all[key] = value
    best_threshold = max(final_scores, key=final_scores.get)
    metrics_all[f'{prefix}_best_threshold'] = best_threshold
    metrics_all[f'{prefix}_best_final_score'] = final_scores[best_threshold]
    # log
    val_loss = [loss_heatmap_im / len(dataloader) * scale_factor[0], loss_score_im / len(dataloader) * scale_factor[1],
                loss_heatmap_mis / len(dataloader), loss_score_mis / len(dataloader) * scale_factor[1]]
    if log_flag:
        wandb.log(metrics_all, step=iter_counter)
        logger.info(json.dumps(metrics_all, indent=4))
        logger.info(f'{prefix}...')
        logger.info(f"Iteration {iter_counter} Validation:\n"
                    f"Implausibility Heatmap Loss = {val_loss[0]}, Implausibility Score Loss = {val_loss[1]}\n"
                    f"Sematic Heatmap Loss = {val_loss[2]}, Misalignment Score Loss = {val_loss[3]}")
        wandb.log({f'{prefix}_heatmap_loss': val_loss[0], f'{prefix}_score_loss': val_loss[1]}, step=iter_counter)
        # print(f"Iteration {iter_counter} Validation:\n"
        #       f"Implausibility Heatmap Loss = {val_loss[0]}, Implausibility Score Loss = {val_loss[1]}\n"
        #       f"Sematic Heatmap Loss = {val_loss[2]}, Misalignment Score Loss = {val_loss[3]}")
    model.train()
    return metrics_all, preds


# TODO: bug to fix, stuck at gathering
@torch.no_grad()
def evaluate_accelerator(model, dataloader, device, criterion, iter_counter, prefix='val', log_flag=True,
                         threshold_list=None, accelerator=None):
    if threshold_list is None:
        threshold_list = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 85, 90, 95, 100, 105, 110]
    model.eval()
    loss_heatmap_im, loss_score_im, loss_heatmap_mis, loss_score_mis = 0, 0, 0, 0
    metrics_all = {}
    preds = {}
    gts = {}
    count = 0
    date_num = 0
    results_list = []
    target_heatmap_gather = None
    target_score_gather = None
    output_heatmap_gather = None
    output_score_gather = None
    with torch.no_grad():
        for inputs, targets in tqdm(dataloader, total=len(dataloader), desc=f'{prefix} evaluation'):
            count += 1
            inputs = inputs.to(device)
            pixel_values = inputs['pixel_values'].squeeze(1)
            if accelerator.state.mixed_precision == 'bf16':
                pixel_values = pixel_values.bfloat16()
            else:
                pixel_values = pixel_values.half()
            outputs_im = model(pixel_values, inputs['input_ids'][:, 0, :])  # implausibility
            output_heatmap, target_heatmap = outputs_im[0].to(device), targets['artifact_map'].to(device)
            output_score, target_score = outputs_im[1].to(device), targets['artifact_score'].to(device)
            if accelerator.state.mixed_precision == 'bf16':
                target_heatmap = target_heatmap.bfloat16()
                target_score = target_score.bfloat16()
            elif accelerator.state.mixed_precision == 'fp16':
                target_heatmap = target_heatmap.half()
                target_score = target_score.half()

            # gather from other gpus
            if target_heatmap_gather is None:
                target_heatmap_gather = target_heatmap.clone()
            else:
                target_heatmap_gather = torch.cat((target_heatmap_gather, target_heatmap), dim=0)
            if target_score_gather is None:
                target_score_gather = target_score.clone()
            else:
                target_score_gather = torch.cat((target_score_gather, target_score), dim=0)
            if output_heatmap_gather is None:
                output_heatmap_gather = output_heatmap.clone()
            else:
                output_heatmap_gather = torch.cat((output_heatmap_gather, output_heatmap), dim=0)
            if output_score_gather is None:
                output_score_gather = output_score.clone()
            else:
                output_score_gather = torch.cat((output_score_gather, output_score), dim=0)
    print('finish evaluation, rank: ', accelerator.state.process_index)
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        print('gathering results')
        padded_target_heatmap = accelerator.pad_across_processes(target_heatmap_gather)
        padded_target_score = accelerator.pad_across_processes(target_score_gather)
        padded_output_heatmap = accelerator.pad_across_processes(output_heatmap_gather)
        padded_output_score = accelerator.pad_across_processes(output_score_gather)

        target_heatmap = accelerator.gather(target_heatmap_gather).cpu()
        target_score = accelerator.gather(target_score_gather).cpu()
        output_heatmap = accelerator.gather(output_heatmap_gather).cpu()
        output_score = accelerator.gather(output_score_gather).cpu()

        print(f'total samples: {output_heatmap.shape[0]}')

        loss_heatmap_im = torch.nn.MSELoss()(output_heatmap, target_heatmap).item()
        loss_score_im = torch.nn.MSELoss()(output_score, target_score).item()

        for idx in range(target_score.shape[0]):
            idx = str(idx)
            preds[idx] = {
                'pred_area': np.squeeze(np.uint8(output_heatmap[idx].float().numpy() * 255)),
                'score': output_score[idx].item(),
            }
            gts[idx] = {
                'pred_area': np.squeeze(np.uint8(target_heatmap[idx].float().numpy() * 255)),
                'score': target_score[idx].item(),
            }

        scale_factor = (255 ** 2, 4)

        metrics_all = {}

        final_scores = {}
        for threshold in threshold_list:
            metrics = metric_cal_online(preds, gts, pred_mask_threshold=threshold)
            for key, value in metrics.items():
                if key == 'final_score':
                    final_scores[threshold] = value
                if 'plcc' not in key.lower() and 'srcc' not in key.lower():
                    key = f'{key}_{threshold}'
                key = f'{prefix}_{key}'
                metrics_all[key] = value
        best_threshold = max(final_scores, key=final_scores.get)
        metrics_all[f'{prefix}_best_threshold'] = best_threshold
        metrics_all[f'{prefix}_best_final_score'] = final_scores[best_threshold]
        # log
        val_loss = [loss_heatmap_im * scale_factor[0], loss_score_im * scale_factor[1],
                    loss_heatmap_mis, loss_score_mis * scale_factor[1]]
        if log_flag:
            wandb.log(metrics_all, step=iter_counter)
            logger.info(json.dumps(metrics_all, indent=4))
            logger.info(f'{prefix}...')
            logger.info(f"Iteration {iter_counter} Validation:\n"
                        f"Implausibility Heatmap Loss = {val_loss[0]}, Implausibility Score Loss = {val_loss[1]}\n"
                        f"Sematic Heatmap Loss = {val_loss[2]}, Misalignment Score Loss = {val_loss[3]}")
            wandb.log({f'{prefix}_heatmap_loss': val_loss[0], f'{prefix}_score_loss': val_loss[1]}, step=iter_counter)
            # print(f"Iteration {iter_counter} Validation:\n"
            #       f"Implausibility Heatmap Loss = {val_loss[0]}, Implausibility Score Loss = {val_loss[1]}\n"
            #       f"Sematic Heatmap Loss = {val_loss[2]}, Misalignment Score Loss = {val_loss[3]}")
    model.train()
    return metrics_all, preds


def main(args):
    # set seed
    if args.seed is not None and args.seed >= 0:
        torch.manual_seed(args.seed)
        torch.cuda.manual_seed(args.seed)
        torch.cuda.manual_seed_all(args.seed)
        np.random.seed(args.seed)
        random.seed(args.seed)
        torch.backends.cudnn.deterministic = True
    # accelerate setup
    accelerator = Accelerator(
        gradient_accumulation_steps=args.accumulate_step, step_scheduler_with_optimizer=False)
    device = accelerator.device
    print = accelerator.print

    # config and log setup
    save_path = f'{args.bytenas_path}/experiments/{args.experiment_name}'
    if not os.path.exists(save_path):
        os.makedirs(save_path, exist_ok=True)
    logging.basicConfig(filename=f'{save_path}/{args.experiment_name}.log', level=logging.INFO,
                        format='%(asctime)s - %(message)s')
    data_path = args.data_path
    with open(f'{save_path}/config.yaml', 'w') as f:
        yaml.dump(vars(args), f)

    # model setup
    processor, model = build_preprocessing_and_model(args)

    if accelerator.is_main_process:
        print(model)
    if len(args.load_checkpoint) > 0:
        print(f'Load checkpoint {args.load_checkpoint}')
        checkpoint = torch.load(f'{args.load_checkpoint}', map_location='cpu')
        model.load_state_dict(checkpoint['model'], strict=True)
    else:
        print('Train from scratch')

    if args.mode == 'eval':
        if dist.get_rank() == 0:
            print('Evaluating...')
            val_dataset = RAHFDatasetCustom(data_path, 'val', processor=processor, img_len=args.input_size, val_split=10000,
                                      mask_size=args.mask_size, prompt_template=args.model, args=args)
            val_dataloader = DataLoader(dataset=val_dataset,
                                        batch_size=args.batch_size,
                                        shuffle=False,
                                        pin_memory=True,
                                        num_workers=12,
                                        sampler=None)
            thresholds = list(range(0, 200, 5))
            metrics, preds = evaluate(model, val_dataloader, device, None, 0, prefix='val', log_flag=False,
                                      threshold_list=thresholds)
            print(json.dumps(metrics, indent=4))
            return

    print('Preparing dataloader...')
    train_dataset = RAHFDatasetCustom(data_path, 'train', processor=processor, finetune=False, img_len=args.input_size,
                                mask_size=args.mask_size,
                                val_split=args.val_split, prompt_template=args.model, args=args)
    train_dataloader = DataLoader(dataset=train_dataset,
                                  batch_size=args.batch_size,
                                  shuffle=True,
                                  num_workers=12,
                                  pin_memory=True,
                                  sampler=None)

    val_dataset = RAHFDatasetCustom(data_path, 'val', processor=processor, img_len=args.input_size, val_split=args.val_split,
                              mask_size=args.mask_size, prompt_template=args.model, args=args)
    val_dataloader = DataLoader(dataset=val_dataset,
                                batch_size=args.batch_size_val,
                                # batch_size=1,   # to get finetune performance
                                shuffle=False,
                                pin_memory=True,
                                num_workers=12,
                                sampler=None)

    if args.epoch > 0:
        args.iters = args.epoch * len(train_dataloader) // accelerator.num_processes // args.accumulate_step
        print('epoch are set, total iter:', args.iters)

    # params group
    backbone_params = []
    head_params = []
    for name, param in model.named_parameters():
        if 'image_encoder' in name or 'text_encoder' in name or 'florence2' in name:
            # if 'lora' not in name:
            backbone_params.append(param)
        else:
            head_params.append(param)

    param_groups = []
    if len(backbone_params) > 0:
        param_groups.append({'params': backbone_params, 'weight_decay': 1e-2, 'lr': args.lr * args.backbone_lr_decay})
    if len(head_params) > 0:
        param_groups.append({'params': head_params, 'lr': args.lr, 'weight_decay': 5e-5})
    optimizer = torch.optim.AdamW(param_groups)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.iters, eta_min=args.min_lr, last_epoch=-1)
    criterion = ScoreLoss(type=args.score_loss)
    criterion_heatmap = MaskLoss(type=args.mask_loss)

    # accelerate setup
    model, optimizer, train_dataloader, scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, scheduler)
    # val_dataloader = accelerator.prepare(val_dataloader)
    ema_model = ExponentialMovingAverage(accelerator.unwrap_model(model), args.ema_decay) if args.ema else None

    if accelerator.is_main_process: wandb.init(project="NTIRE2025", name=args.experiment_name)
    accelerator.print(optimizer)
    accelerator.print(scheduler)

    def train_loop(model, train_dataloader, val_dataloader, iter_counter, epoch_counter, end_iter, stage='training'):
        while True:
            model.train()
            print(f'Epoch {epoch_counter}')
            bar = tqdm(total=len(train_dataloader),
                       desc=f"stage: {stage}, iter:{iter_counter}, epoch:{epoch_counter}, end:{end_iter}")
            for batch_id, (inputs, targets) in enumerate(train_dataloader):
                if accelerator.is_main_process:
                    bar.update()
                # with accelerator.accumulate(model):
                with accelerator.autocast():
                    inputs = inputs
                    inputs_pixel_values, inputs_ids_im, inputs_ids_mis = inputs['pixel_values'].squeeze(1), inputs[
                                                                                                                'input_ids'][
                                                                                                            :, 0,
                                                                                                            :], \
                        inputs['input_ids'][:, 1, :]
                    if accelerator.state.mixed_precision == 'bf16':
                        inputs_pixel_values = inputs_pixel_values.bfloat16()
                    elif accelerator.state.mixed_precision == 'fp16':
                        inputs_pixel_values = inputs_pixel_values.half()
                    outputs_im = model(inputs_pixel_values, inputs_ids_im, need_score=True)  # implausibility
                    output_heatmap, target_heatmap = outputs_im[0], targets['artifact_map']
                    output_score, target_score = outputs_im[1], targets['artifact_score']
                    # get accelerator type
                    if accelerator.state.mixed_precision == 'bf16':
                        target_heatmap = target_heatmap.bfloat16()
                        target_score = target_score.bfloat16()
                    elif accelerator.state.mixed_precision == 'fp16':
                        target_heatmap = target_heatmap.half()
                        target_score = target_score.half()
                    loss_im = criterion_heatmap(output_heatmap, target_heatmap)
                    loss_score = criterion(output_score, target_score)
                    loss = loss_im * args.mask_loss_weight + loss_score * args.score_loss_weight
                    accelerator.backward(loss)
                if iter_counter % args.accumulate_step == 0 or batch_id == len(train_dataloader):
                    # if accelerator.sync_gradients:
                    #     accelerator.clip_grad_norm_(model.parameters(), 2.)
                    optimizer.step()
                    if stage != 'warmup': scheduler.step()
                    optimizer.zero_grad()
                    if ema_model is not None:
                        ema_model.update()
                iter_counter += 1
                if iter_counter % args.log_step == 0:
                    iter_loss = [[], [], [], []]
                    iter_loss[0].append(loss_im.item())
                    iter_loss[1].append(loss_score.item())
                    iter_loss[2].append(0)
                    iter_loss[3].append(0)
                    if accelerator.is_main_process:
                        print_log_iter(optimizer, iter_counter, iter_loss, logger)
                    accelerator.wait_for_everyone()
                if iter_counter >= end_iter:
                    return iter_counter, epoch_counter
            # after every epoch
            if args.val_iter > 0 and epoch_counter % args.val_iter == 0:
                if accelerator.is_main_process:
                    if ema_model is not None:
                        ema_model.apply_shadow()
                    if ema_model is not None: ema_model.restore()
                # TODO: run on all gpu to avoid processes crush
                accelerator.wait_for_everyone()
                evaluate(accelerator.unwrap_model(model), val_dataloader, accelerator.device, criterion,
                         iter_counter, prefix='val', accelerator=accelerator,
                         log_flag=True if accelerator.is_main_process else False)
                # evaluate_accelerator(accelerator.unwrap_model(model), val_dataloader, accelerator.device, criterion,
                #                      iter_counter, prefix='val', accelerator=accelerator,
                #                      log_flag=True)
            if args.save_iter > 0 and epoch_counter % args.save_iter == 0:
                if accelerator.is_main_process:
                    # save model and tag with iter tag
                    to_save_model = accelerator.unwrap_model(model)
                    if ema_model is not None: ema_model.apply_shadow()
                    checkpoint = {
                        'model': to_save_model.state_dict(),
                        # 'optimizer': optimizer.state_dict(),
                        # 'scheduler': scheduler.state_dict()
                    }
                    torch.save(checkpoint, f'{save_path}/{iter_counter}.pth')
                    torch.save(checkpoint, f'{save_path}/last.pth')
                    if ema_model is not None: ema_model.restore()
                accelerator.wait_for_everyone()
            epoch_counter += 1

    iter_counter = 0
    epoch_counter = 0
    warmup_step = 0
    # warmup training
    if args.warmup_epoch > 0:
        if args.use_lora and len(args.load_checkpoint) < 1:
            raise ValueError('warmup stage does not support lora if checkpoint is not provided')
        set_params_trainable(accelerator.unwrap_model(model), args, stage='warmup')
        if accelerator.is_main_process:
            print('warmup Training...')
            print('warmup stage training, freeze image&text encoder')
            print_trainable_params(accelerator.unwrap_model(model))
        train_dataloader_warmup = DataLoader(dataset=train_dataset,
                                             batch_size=args.batch_size_warmup,
                                             shuffle=True,
                                             num_workers=12,
                                             pin_memory=True,
                                             sampler=None)
        train_dataloader_warmup = accelerator.prepare(train_dataloader_warmup)
        warmup_step = len(train_dataloader_warmup) * args.warmup_epoch
        iter_counter, epoch_counter = train_loop(model, train_dataloader_warmup, val_dataloader, iter_counter,
                                                 epoch_counter, warmup_step, stage='warmup')
        del train_dataloader_warmup
    # after warmup training
    set_params_trainable(accelerator.unwrap_model(model), args, stage='train')
    if accelerator.is_main_process:
        print('after warmup(if have) training')
        print_trainable_params(accelerator.unwrap_model(model))
    iter_counter, epoch_counter = train_loop(model, train_dataloader, val_dataloader, iter_counter, epoch_counter,
                                             args.iters + warmup_step)
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        # accelerator.save_model(model, save_path)
        if ema_model is not None: ema_model.apply_shadow()
        to_save_model = accelerator.unwrap_model(model)
        checkpoint = {
            'model': to_save_model.state_dict(),
            # 'optimizer': optimizer.state_dict(),
            # 'scheduler': scheduler.state_dict()
        }
        torch.save(checkpoint, f'{save_path}/{iter_counter}.pth')
        torch.save(checkpoint, f'{save_path}/last.pth')


if __name__ == '__main__':
    from configs import get_args

    args = get_args()
    main(args)
